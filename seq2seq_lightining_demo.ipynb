{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-04-09 15:38:31--  https://www.manythings.org/anki/rus-eng.zip\n",
      "Распознаётся www.manythings.org (www.manythings.org)… 173.254.30.110\n",
      "Подключение к www.manythings.org (www.manythings.org)|173.254.30.110|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 15460248 (15M) [application/zip]\n",
      "Сохранение в: «rus-eng.zip»\n",
      "\n",
      "rus-eng.zip         100%[===================>]  14,74M  41,5KB/s    за 3m 2s   \n",
      "\n",
      "2023-04-09 15:41:35 (82,8 KB/s) - «rus-eng.zip» сохранён [15460248/15460248]\n",
      "\n",
      "Archive:  rus-eng.zip\n",
      "  inflating: rus.txt                 \n",
      "  inflating: _about.txt              \n"
     ]
    }
   ],
   "source": [
    "!wget https://www.manythings.org/anki/rus-eng.zip\n",
    "!unzip rus-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models.seq2seq_rnn' from '/home/somov-od/mashine_translation/./src/models/seq2seq_rnn.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import sys, os\n",
    "import importlib\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"./src\"))\n",
    "\n",
    "from models import seq2seq_rnn\n",
    "from data.datamodule import DataManager\n",
    "\n",
    "\n",
    "importlib.reload(seq2seq_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_prefixes = (\n",
    "    \"i am \",\n",
    "    \"i m \",\n",
    "    \"he is\",\n",
    "    \"he s \",\n",
    "    \"she is\",\n",
    "    \"she s \",\n",
    "    \"you are\",\n",
    "    \"you re \",\n",
    "    \"we are\",\n",
    "    \"we re \",\n",
    "    \"they are\",\n",
    "    \"they re \",\n",
    ")\n",
    "\n",
    "def filter_func(x):\n",
    "    MAX_LENGTH = 5\n",
    "    len_filter = lambda x: len(x[0].split(\" \")) <= MAX_LENGTH and len(x[1].split(\" \")) <= MAX_LENGTH\n",
    "    prefix_filter = lambda x: x[0].startswith(eng_prefixes)\n",
    "    return len_filter(x) and prefix_filter(x)\n",
    "\n",
    "config = {\n",
    "    \"batch_size\": 128,          # <--- size of batch\n",
    "    \"num_workers\": 8,          # <--- num cpu to use in dataloader\n",
    "    \"filter\": filter_func,      # <--- callable obj to filter data  \n",
    "    \"filename\": \"./rus.txt\",    # <--- path to file with sentneces\n",
    "    \"lang1\": \"en\",              # <--- name of the first lang    \n",
    "    \"lang2\": \"ru\",              # <--- name of the second lang\n",
    "    \"reverse\": False,           # <--- direct or reverse order in pairs\n",
    "    \"train_size\": 0.8,          # <--- ratio of data pairs to use in train\n",
    "    \"run_name\": \"tutorial\",     # <--- run name to logger and checkpoints\n",
    "    \"quantile\": 0.95,           # <--- (1 - quantile) longest sentences will be removed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from file: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 467119/467119 [00:06<00:00, 75847.90it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data manager\n",
    "dm = DataManager(config)\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "input_lang_n_words = dm.input_lang_n_words\n",
    "output_lang_n_words = dm.output_lang_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Представление данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you re a filthy liar', 'ты грязная лгунья'),\n",
       " ('they are both good', 'они оба хорошие'),\n",
       " ('i m ill', 'я болен'),\n",
       " ('i m young and innocent', 'я молода и невинна'),\n",
       " ('he s a liar', 'он лгун')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[  0],\n",
       "         [132],\n",
       "         [ 68],\n",
       "         [ 22],\n",
       "         [  1]]),\n",
       " tensor([[  0],\n",
       "         [298],\n",
       "         [201],\n",
       "         [642],\n",
       "         [  1]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.output_lang.word2index['лгунья']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = seq2seq_rnn.Seq2SeqRNN(\n",
    "    encoder_vocab_size=input_lang_n_words,\n",
    "    encoder_embedding_size=256,\n",
    "    decoder_embedding_size=256,\n",
    "    decoder_vocab_size=output_lang_n_words,\n",
    "    lr=1e-3,\n",
    "    output_lang_index2word=dm.train_dataset.output_lang.index2word,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/somov-od/nmt_venv/lib/python3.8/site-packages/lightning_fabric/connector.py:562: UserWarning: 16 is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "  rank_zero_warn(\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(overfit_batches=1)` was configured so 1 batch will be used.\n"
     ]
    }
   ],
   "source": [
    "# TB Logger\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=config[\"run_name\"])\n",
    "\n",
    "# Callbacks\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    save_top_k=3,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    dirpath=\"runs/{}/\".format(config[\"run_name\"]),\n",
    "    filename=\"{epoch:02d}-{step:d}-{val_loss:.4f}\",\n",
    "    verbose=True,\n",
    "    every_n_epochs=1,\n",
    ")\n",
    "lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "# Initialize a Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator='cuda',\n",
    "    overfit_batches=1,\n",
    "    devices=[0],\n",
    "    precision=16,\n",
    "    max_epochs=50,\n",
    "    min_epochs=1,\n",
    "    callbacks=[lr_monitor, checkpoint_callback],\n",
    "    check_val_every_n_epoch=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading from file:  64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                           | 298621/467119 [00:03<00:02, 74069.97it/s]"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see tensorboard logs in two ways: launch tensorboard extension in jupyter notebook or use CLI method.\n",
    "\n",
    "CLI:\n",
    "1. Tap `tensorboard --logdir=./lightning_logs --port=6006`\n",
    "2. Forward selected port in the ssh connection if you are working remote else just open `localhost:6006` in the browser\n",
    "\n",
    "\n",
    "Jupyter:\n",
    "1. Load extension: `%load_ext tensorboard`\n",
    "2. Launch built-in tensorboard: `%tensorboard --logdir=./lightning_logs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboardX module is not an IPython extension.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboardX\n",
    "%tensorboard --logdir=./lightning_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints\n",
    "\n",
    "#### Load model from checkpoint\n",
    "Using `self.save_hyperparameters` in `__init__` body of `pl.LightningModule` allows to load model in this way:\n",
    "```python\n",
    "import pytorch_lightning as pl\n",
    "model = Seq2SeqRNN.load_from_checkpoint('checkpoint.ckpt')\n",
    "```\n",
    "\n",
    "Or you can load checkpoint in natural pytorch way:\n",
    "```python\n",
    "model = Seq2SeqRNN(*args, **kwargs)\n",
    "model.load_state_dict(torch.load('checkpoint.ckpt')['state_dict'])\n",
    "```\n",
    "\n",
    "#### Add custom metrics to logger\n",
    "https://lightning.ai/docs/pytorch/stable/extensions/logging.html\n",
    "\n",
    "\n",
    "#### Enable grad accumulation\n",
    "In this example accumulation will be the following: \n",
    "1. from 0 to 15th epoch accumulate 4 batches\n",
    "2. from 15th to 25th epoch accumulate 2 batches\n",
    "3. from 25th epoch accumulate 1 batch\n",
    "\n",
    "```python\n",
    "from pytorch_lightning.callbacks import GradientAccumulationScheduler\n",
    "\n",
    "accumulator = GradientAccumulationScheduler(scheduling={0: 4, 15: 2, 25: 1})\n",
    "trainer = pl.Trainer(\n",
    "    ...\n",
    "    callbacks=[..., accumulator],\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "#### Configure learning rate scheduler\n",
    "\n",
    "```python\n",
    "def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.1,\n",
    "        patience=0,\n",
    "        threshold=1e-2,\n",
    "        threshold_mode='rel',\n",
    "        cooldown=0,\n",
    "        min_lr=0,\n",
    "        eps=1e-09,\n",
    "        verbose=True\n",
    "    )\n",
    "    lr_dict = {\n",
    "        \"scheduler\": lr_scheduler,\n",
    "        \"interval\": \"epoch\",\n",
    "        \"frequency\": 1,\n",
    "        \"monitor\": \"val_loss\"\n",
    "    }\n",
    "    return [optimizer], [lr_dict]\n",
    "```\n",
    "\n",
    "#### Other logger: WandB\n",
    "You can use famous weights&biases logger which is natively supports in pytorch-lightning:\n",
    "https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.loggers.wandb.html#module-lightning.pytorch.loggers.wandb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nmt_venv",
   "language": "python",
   "name": "nmt_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}